{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235747/codeshare/3054?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGA torch 호환성 체크.\n",
    "\n",
    "t = torch.Tensor(3,4).cuda()\n",
    "t.device\n",
    "# 실행이 잘 되면 GPU 잘 쓸 수 있는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
    "test_data = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
    "\n",
    "topic_dict = pd.read_csv(os.path.join(PATH, 'topic_dict.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('index', axis=1)\n",
    "test_data = test_data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eunjeon import Mecab\n",
    "# from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data를 token과 label로 분리해 numpy로 반환해준다.\n",
    "def convert_data(tokenizer, data_df, case):\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # tokenize\n",
    "        token = tokenizer.encode(data_df['title'][i], max_length=MAX_LEN, padding='max_length', truncation=True)[1:-1]\n",
    "        \n",
    "        # making segment\n",
    "        # segment = [0]*MAX_LEN\n",
    "        \n",
    "        # token, segment\n",
    "        tokens.append(token)\n",
    "        # segments.append(segments)\n",
    "        \n",
    "        if case == 'train':\n",
    "            targets.append(data_df['topic_idx'][i])\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    \n",
    "    if case == 'train':\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        return [tokens, segments], targets\n",
    "    \n",
    "    return [tokens, segments],\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_seq_len, pad_idx, mode):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "        if mode=='train' or mode=='eval':\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise Exception(f'\\'--mode\\' should be \\'train\\' or \\'eval\\'. But your arg is \\'{mode}\\'')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.tokenizer.encode(self.data['title'][idx])[1:-1]\n",
    "        con_len = len(context)\n",
    "        pad_li = [self.pad_idx]*(self.max_len - con_len)\n",
    "        context += pad_li\n",
    "        context = torch.LongTensor(context)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            label_idx = int(self.data['topic_idx'][idx])\n",
    "            label = torch.zeros(7)\n",
    "            label[label_idx] += 1\n",
    "            \n",
    "            return context, label\n",
    "        return context,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 30\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "# make dataset\n",
    "train_dataset = CustomDataset('./dataset/train_data.csv', tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset('./dataset/test_data.csv', tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for X, in test_loader:\n",
    "    count+=1\n",
    "print(count, count*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 의 encdoer n개를 통과시켜, 마지막 output중 첫번째 token이 Affine Layer을 지나는 model이다.\n",
    "\n",
    "- d_model = 512\n",
    "- d_word_vec = 512\n",
    "- n_layers = 6\n",
    "- n_head = 8\n",
    "- d_inner = 2048\n",
    "- dropout = 0.1\n",
    "- n_position = 50\n",
    "- pad_idx = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from model.Models import Encoder\n",
    "from einops import rearrange\n",
    "\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, pad_idx, d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                 n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=50, num_labels=7,max_seq_len=30):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_src_vocab=n_vocab, d_word_vec=d_word_vec, n_layers=n_layers, \n",
    "                               d_inner=d_inner, n_head=n_head, d_k=d_k, d_v=d_k, d_model=d_model, \n",
    "                               pad_idx=pad_idx, dropout=dropout, n_position=n_position, scale_emb=False)\n",
    "        self.linear1 = nn.Linear(max_seq_len*d_model, d_model, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_model, num_labels, bias=True)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, context):\n",
    "        \n",
    "        mask = get_pad_mask(context, self.pad_idx)\n",
    "        enc_output, *_ = self.encoder(context, mask)\n",
    "        lin_output = self.relu(self.linear1(rearrange(enc_output,'b s d -> b (s d)')))\n",
    "        seq_logit = self.relu(self.linear2(lin_output))\n",
    "        \n",
    "        return seq_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 5: 모델 반환모양이 ( b seq class ) 이다. ( b c ) 이어야하는데..?\n",
    "\n",
    "in model forward... context.shape torch.Size([64, 30])\n",
    "in model forward... mask.shape torch.Size([64, 1, 30])\n",
    "in model forward... enc_output.shape torch.Size([64, 30, 512])      # linear 을 잘못 수행하였다.\n",
    "in model forward... after_linear.shape torch.Size([64, 30, 7])\n",
    "-> linear 추가해서, 한번 더 거치게 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선점 1\n",
    "\n",
    "output에 softmax 안했는데, 어떤 영향이 있는거지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    \"BATCH_SIZE\":64,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'], \n",
    "                                        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                        n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "# optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)\n",
    "optimizer = torch.optim.Adam(params = classifier_model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, optimizer, device, criterion):\n",
    "    model.to(device)\n",
    "    criterion = criterion.to(device)   # loss\n",
    "    # metric = pass     # TODO 1: find appropriate metric\n",
    "\n",
    "    for epoch in range(1,CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        # train_metric = [] # TODO 1\n",
    "\n",
    "        for X,Y in tqdm(train_loader, desc='Training ...'):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            with torch.no_grad():\n",
    "                # metric_name = metric  # TODO 1\n",
    "                pass\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            # train_metric.append(metric_name.item())   # TODO 1\n",
    "        \n",
    "        print(f\"Epoch {epoch}  |  Train Loss : [{np.mean(train_loss):.5f}]  |  Train Metric: not yet \")   # TODO 1\n",
    "        \n",
    "        # TODO 2: 지금 epoch의 metric이 더 좋으면, 더 좋은 모델을 best model에 저장한다.\n",
    "        pass\n",
    "        \n",
    "                \n",
    "train(classifier_model, train_loader, optimizer, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 1 : nn.embedding() 은 input Tensor가 int 형이어야한다.\n",
    "c:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23 in TransformerClassifier.forward(self, context)\n",
    "...\n",
    "   2197     # remove once script supports set_grad_enabled\n",
    "   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
    "-> 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
    "\n",
    "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
    "\n",
    "------------------------\n",
    "\n",
    "학습 해보려 하는데,\n",
    "- 모델 내부에서 int로 받아야하는 'indices'라는 놈에 floatTensor 가 들어가고있어 말썽인 상황이다. 내일 해결해봐야지.\n",
    "- torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 의 첫번째 argument weight가 Int이어야하는데 Float로 들어온건가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 2: \n",
    "-> 3014 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
    "\n",
    "IndexError: Target -9223372036854775808 is out of bounds.\n",
    "\n",
    "model 어딘가에서 normalization 일어나고있지 않나?\n",
    "linear 결과에 norm 추가해야하나? 다른 모델 봐야겠다.\n",
    "- Y 찍어보니 그 안에 엄청 큰 절댓값이 존재한다.. 뭐지?\n",
    "\n",
    "-1.9357e+16 이 값이, -19356592969351168 이 된다. 왜 저 값이 존재하지?\n",
    "Y label 만드는 과정에서 오류가 있을거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 3: model 과 Tensor 의 .to(device) 함수 차이\n",
    "https://stackoverflow.com/questions/59560043/what-is-the-difference-between-model-todevice-and-model-model-todevice\n",
    "\n",
    "---> 36 train(classifier_model, train_loader, optimizer, device, criterion)\n",
    "AttributeError: 'TransformerClassifier' object has no attribute 'device'\n",
    "\n",
    "Model can be placed in GPU with code,\n",
    "```\n",
    "a = my_model()  # a is in cpu\n",
    "a.to(device)    # a is moved to gpu\n",
    "```\n",
    "\n",
    "But Tensor cannot be moved to GPU with the same code.\n",
    "```\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.to(device)    # a is in cpu\n",
    "a = a.to(devivce)   # a is now in gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 4: \n",
    "RuntimeError: CUDA error: no kernel image is available for execution on the device\n",
    "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
    "\n",
    "https://ndb796.tistory.com/744  을 보고 해결했다.\n",
    "- 현재 커널의 python 버전과, CUDA 버전이 호환되지 않아서 발생하는 문제다.\n",
    "- python 버전을 3.6 -> 3.8 업그레이드함으로 해결했다.(사실은 conda 환경을 새로 팜)\n",
    "    - 내가 가진 CUDA버전이 3.6이랑 호환이 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        for X, in tqdm(test_loader, desc='Inferencing ...'):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            preds += pred.cpu().tolist()\n",
    "    \n",
    "    # 전체 prediction 된 것들을 numpy로 한번에 반환해준다.\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: 가장 좋은 모델 ckpt 가져와서 load 해줘야하지만 지금은 대충 train 한거 기억하는거 그대로 가져온다.\n",
    "# 대충 하는중\n",
    "model = classifier_model\n",
    "preds = predict(model, test_loader, device)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/kist/Desktop/JH/DACON_transformer/test_preds', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test_data output checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.load('/home/kist/Desktop/JH/DACON_transformer/test_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과, max 해서 label로 반환\n",
    "\n",
    "pred_outputs=[]\n",
    "for i in range(len(preds)):\n",
    "    pred_outputs.append(np.argmax(preds[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output inference\n",
    "import pandas as pd\n",
    "\n",
    "print(pd.read_csv('./dataset/topic_dict.csv'))\n",
    "print(pd.read_csv('./dataset/test_data.csv'))\n",
    "print(pred_outputs[:10])\n",
    "\"\"\"\n",
    "얼추 잘 맞는것같기도 하자. 따로 자세히 보자.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_sub = pd.read_csv('./dataset/sample_submission.csv')\n",
    "real_sub = pd.DataFrame(sample_sub['index'])\n",
    "\n",
    "real_sub['topic_idx'] = pred_outputs\n",
    "\n",
    "\"\"\"\n",
    "Q. 혹시 test dataset에서 'shuffle=False' 하면 순서대로 반영되는게 맞는건가?\n",
    "\"\"\"\n",
    "real_sub.to_csv('first_submission_TransformerEncModel.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a314cea1fe53c45a6bb2b7511b9db63e391b2a283688b4693c5ca4fc08ea7d82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
