{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235747/codeshare/3054?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
    "test_data = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
    "\n",
    "topic_dict = pd.read_csv(os.path.join(PATH, 'topic_dict.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('index', axis=1)\n",
    "test_data = test_data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45649</th>\n",
       "      <td>KB금융 미국 IB 스티펠과 제휴…선진국 시장 공략</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>1보 서울시교육청 신종코로나 확산에 개학 연기·휴업 검토</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>게시판 키움증권 2020 키움 영웅전 실전투자대회</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45652</th>\n",
       "      <td>답변하는 배기동 국립중앙박물관장</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45653</th>\n",
       "      <td>2020 한국인터넷기자상 시상식 내달 1일 개최…특별상 김성후</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45654 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  topic_idx\n",
       "0                인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1          실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2          이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3        NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4               시진핑 트럼프에 중미 무역협상 조속 타결 희망          4\n",
       "...                                   ...        ...\n",
       "45649        KB금융 미국 IB 스티펠과 제휴…선진국 시장 공략          1\n",
       "45650     1보 서울시교육청 신종코로나 확산에 개학 연기·휴업 검토          2\n",
       "45651         게시판 키움증권 2020 키움 영웅전 실전투자대회          1\n",
       "45652                   답변하는 배기동 국립중앙박물관장          2\n",
       "45653  2020 한국인터넷기자상 시상식 내달 1일 개최…특별상 김성후          2\n",
       "\n",
       "[45654 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from eunjeon import Mecab\n",
    "# from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인천→핀란드 항공기 결항…휴가철 여행객 분통'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data를 token과 label로 분리해 numpy로 반환해준다.\n",
    "def convert_data(tokenizer, data_df, case):\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # tokenize\n",
    "        token = tokenizer.encode(data_df['title'][i], max_length=MAX_LEN, padding='max_length', truncation=True)[1:-1]\n",
    "        \n",
    "        # making segment\n",
    "        # segment = [0]*MAX_LEN\n",
    "        \n",
    "        # token, segment\n",
    "        tokens.append(token)\n",
    "        # segments.append(segments)\n",
    "        \n",
    "        if case == 'train':\n",
    "            targets.append(data_df['topic_idx'][i])\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    \n",
    "    if case == 'train':\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        return [tokens, segments], targets\n",
    "    \n",
    "    return [tokens, segments],\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_seq_len, pad_idx):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.tokenizer.encode(self.data['title'][idx])[1:-1]\n",
    "        label_idx = int(self.data['topic_idx'][idx])\n",
    "        con_len = len(context)\n",
    "        pad_li = [self.pad_idx]*(self.max_len - con_len)\n",
    "        context += pad_li\n",
    "        \n",
    "        context = torch.Tensor(context)\n",
    "        label = torch.empty(7)\n",
    "        label[label_idx] += 1\n",
    "        \n",
    "        return context, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 30\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "# make dataset\n",
    "train_dataset = CustomDataset('./dataset/train_data.csv', tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset('./dataset/test_data.csv', tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tensor([4.2039e-45, 0.0000e+00, 1.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00,\n",
      "        8.4078e-45])\n"
     ]
    }
   ],
   "source": [
    "# 64개의 token seq랑, label 들을 반환한다.\n",
    "for batch in train_loader:\n",
    "    print(len(batch[0]))\n",
    "    print(batch[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 의 encdoer n개를 통과시켜 Affine Layer을 지나는 model이다.\n",
    "\n",
    "- d_model = 512\n",
    "- d_word_vec = 512\n",
    "- n_layers = 6\n",
    "- n_head = 8\n",
    "- d_inner = 2048\n",
    "- dropout = 0.1\n",
    "- n_position = 50\n",
    "- pad_idx = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from model.Models import Encoder\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, pad_idx, d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                 n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=50, num_labels=7):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_src_vocab=n_vocab, d_word_vec=d_word_vec, n_layers=n_layers, \n",
    "                               d_inner=d_inner, n_head=n_head, d_k=d_k, d_v=d_k, d_model=d_model, \n",
    "                               pad_idx=pad_idx, dropout=dropout, n_position=n_position, scale_emb=False)\n",
    "        self.linear = nn.Linear(d_model,num_labels,bias=True)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, context):\n",
    "\n",
    "        mask = get_pad_mask(context, self.pad_idx)\n",
    "        \n",
    "        enc_output, *_ = self.encoder(context, mask)\n",
    "        seq_logit = self.linear(enc_output)\n",
    "        \n",
    "        return seq_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    \"BATCH_SIZE\":64,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "device = 'cpu'\n",
    "classifier_model = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'], \n",
    "                                        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                        n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "# optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)\n",
    "optimizer = torch.optim.Adam(params = classifier_model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m]  |  Train Loss : [\u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(train_loss)\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]  |  Train Metric: not yet \u001b[39m\u001b[39m\"\u001b[39m)   \u001b[39m# TODO 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         \u001b[39m# TODO 2: 지금 epoch의 metric이 더 좋으면, 더 좋은 모델을 best model에 저장한다.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m train(classifier_model, train_loader, optimizer, device, criterion)\n",
      "\u001b[1;32mc:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, device, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m Y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, Y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# metric_name = metric  # TODO 1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23\u001b[0m in \u001b[0;36mTransformerClassifier.forward\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, context):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     mask \u001b[39m=\u001b[39m get_pad_mask(context, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_idx)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     enc_output, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     seq_logit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(enc_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jaeng/Desktop/VSC/DACON/text_classification/main.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m seq_logit\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\model\\Models.py:83\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src_seq, src_mask, return_attns)\u001b[0m\n\u001b[0;32m     80\u001b[0m enc_slf_attn_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     82\u001b[0m \u001b[39m# input embedding, positional encoding\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m enc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_word_emb(src_seq)\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_emb:\n\u001b[0;32m     85\u001b[0m     enc_output \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\jaeng\\anaconda3\\envs\\Ethics-Korean\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader, optimizer, device, criterion):\n",
    "    model.to(device)\n",
    "    criterion = criterion.to(device)   # loss\n",
    "    # metric = pass     # TODO 1: find appropriate metric\n",
    "    \n",
    "    for epoch in range(CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_metric = []        \n",
    "\n",
    "        for X,Y in train_loader:\n",
    "            X.to(device)\n",
    "            Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            with torch.no_grad():\n",
    "                # metric_name = metric  # TODO 1\n",
    "                pass\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            # train_metric.append(metric_name.item())   # TODO 1\n",
    "        print(f\"Epoch [{epoch}]  |  Train Loss : [{np.mean(train_loss):.5f}]  |  Train Metric: not yet \")   # TODO 1\n",
    "        \n",
    "        # TODO 2: 지금 epoch의 metric이 더 좋으면, 더 좋은 모델을 best model에 저장한다.\n",
    "            \n",
    "        \n",
    "                \n",
    "train(classifier_model, train_loader, optimizer, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23 in TransformerClassifier.forward(self, context)\n",
    "...\n",
    "   2197     # remove once script supports set_grad_enabled\n",
    "   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
    "-> 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
    "\n",
    "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
    "\n",
    "------------------------\n",
    "\n",
    "학습 해보려 하는데, 모델 내부에서 int로 받아야하는 'indices'라는 놈에 floatTensor 가 들어가고있어 말썽인 상황이다. 내일 해결해봐야지.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Ethics-Korean')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7b8a9d296651b950055feafaab8f47ff1cee70807e6c570958d0b27d2589bac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
