{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235747/codeshare/3054?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kist/anaconda3/envs/dacon/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
    "test_data = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
    "\n",
    "topic_dict = pd.read_csv(os.path.join(PATH, 'topic_dict.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45649</th>\n",
       "      <td>45649</td>\n",
       "      <td>KB금융 미국 IB 스티펠과 제휴…선진국 시장 공략</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>45650</td>\n",
       "      <td>1보 서울시교육청 신종코로나 확산에 개학 연기·휴업 검토</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>45651</td>\n",
       "      <td>게시판 키움증권 2020 키움 영웅전 실전투자대회</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45652</th>\n",
       "      <td>45652</td>\n",
       "      <td>답변하는 배기동 국립중앙박물관장</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45653</th>\n",
       "      <td>45653</td>\n",
       "      <td>2020 한국인터넷기자상 시상식 내달 1일 개최…특별상 김성후</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45654 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                               title  topic_idx\n",
       "0          0            인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1          1      실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2          2      이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3          3    NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4          4           시진핑 트럼프에 중미 무역협상 조속 타결 희망          4\n",
       "...      ...                                 ...        ...\n",
       "45649  45649        KB금융 미국 IB 스티펠과 제휴…선진국 시장 공략          1\n",
       "45650  45650     1보 서울시교육청 신종코로나 확산에 개학 연기·휴업 검토          2\n",
       "45651  45651         게시판 키움증권 2020 키움 영웅전 실전투자대회          1\n",
       "45652  45652                   답변하는 배기동 국립중앙박물관장          2\n",
       "45653  45653  2020 한국인터넷기자상 시상식 내달 1일 개최…특별상 김성후          2\n",
       "\n",
       "[45654 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('index', axis=1)\n",
    "test_data = test_data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eunjeon import Mecab\n",
    "# from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class when using SKF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, max_seq_len, pad_idx, mode):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_idx = pad_idx\n",
    "        if mode=='test':\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            self.mode = 'train'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.tokenizer.encode(self.data['title'][idx])[1:-1]\n",
    "        con_len = len(context)\n",
    "        pad_li = [self.pad_idx]*(self.max_len - con_len)\n",
    "        context += pad_li\n",
    "        context = torch.LongTensor(context)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            label_idx = int(self.data['topic_idx'][idx])\n",
    "            label = torch.zeros(7)\n",
    "            label[label_idx] += 1\n",
    "            return context, label\n",
    "        return context,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold\n",
    "\n",
    "- 데이터셋의 레이블 분포에 차이가 있기에 이를 고려한 Startified K-Fold 를 해준다.\n",
    "- Startified-5-Fold 를 이용하여 각 80%의 train_data로 학습한 모델 5개를 test데이터에 대하여 앙상블한 것을 최종 결과로 채택한다.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold     # GT를 자동인식해서 맞게 분배한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation, StratifiedKfold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "\n",
    "# label에 비례하는 idx를 만든다.\n",
    "for train_idx, valid_idx in skf.split(train_data, train_data['topic_idx']):\n",
    "    train_idx = np.array(train_idx)\n",
    "    valid_idx = np.array(valid_idx)\n",
    "    folds.append((train_idx, valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 나누기\n",
    "train_data0 = train_data.iloc[folds[0][0]].reset_index(drop=True)\n",
    "train_data1 = train_data.iloc[folds[1][0]].reset_index(drop=True)\n",
    "train_data2 = train_data.iloc[folds[2][0]].reset_index(drop=True)\n",
    "train_data3 = train_data.iloc[folds[3][0]].reset_index(drop=True)\n",
    "train_data4 = train_data.iloc[folds[4][0]].reset_index(drop=True)\n",
    "\n",
    "val_data0 = train_data.iloc[folds[0][1]].reset_index(drop=True)\n",
    "val_data1 = train_data.iloc[folds[1][1]].reset_index(drop=True)\n",
    "val_data2 = train_data.iloc[folds[2][1]].reset_index(drop=True)\n",
    "val_data3 = train_data.iloc[folds[3][1]].reset_index(drop=True)\n",
    "val_data4 = train_data.iloc[folds[4][1]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 30\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "# make dataset\n",
    "train_dataset=[]\n",
    "train_dataset.append(MyDataset(data_df=train_data0, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train'))\n",
    "train_dataset.append(MyDataset(data_df=train_data1, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train'))\n",
    "train_dataset.append(MyDataset(data_df=train_data2, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train'))\n",
    "train_dataset.append(MyDataset(data_df=train_data3, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train'))\n",
    "train_dataset.append(MyDataset(data_df=train_data4, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='train'))\n",
    "\n",
    "val_dataset=[]\n",
    "val_dataset.append(MyDataset(data_df=val_data0, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval'))\n",
    "val_dataset.append(MyDataset(data_df=val_data1, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval'))\n",
    "val_dataset.append(MyDataset(data_df=val_data2, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval'))\n",
    "val_dataset.append(MyDataset(data_df=val_data3, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval'))\n",
    "val_dataset.append(MyDataset(data_df=val_data4, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='eval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "# MAX_LEN = 30\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "# train_dataset = MyDataset(\n",
    "#     data_df=train_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_seq_len=MAX_LEN,\n",
    "#     pad_idx=1,\n",
    "#     mode='train'\n",
    "# )\n",
    "# test_dataset = MyDataset(\n",
    "#     data_df=test_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_seq_len=MAX_LEN,\n",
    "#     pad_idx=1,\n",
    "#     mode='test'\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 의 encdoer n개를 통과시켜, 마지막 output중 첫번째 token이 Affine Layer을 지나는 model이다.\n",
    "\n",
    "- d_model = 512\n",
    "- d_word_vec = 512\n",
    "- n_layers = 6\n",
    "- n_head = 8\n",
    "- d_inner = 2048\n",
    "- dropout = 0.1\n",
    "- n_position = 50\n",
    "- pad_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from model.Models import Encoder\n",
    "from einops import rearrange\n",
    "\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, pad_idx, d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                 n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=50, num_labels=7,max_seq_len=30):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_src_vocab=n_vocab, d_word_vec=d_word_vec, n_layers=n_layers, \n",
    "                               d_inner=d_inner, n_head=n_head, d_k=d_k, d_v=d_k, d_model=d_model, \n",
    "                               pad_idx=pad_idx, dropout=dropout, n_position=n_position, scale_emb=False)\n",
    "        self.linear1 = nn.Linear(max_seq_len*d_model, d_model, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_model, num_labels, bias=True)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, context):\n",
    "        \n",
    "        mask = get_pad_mask(context, self.pad_idx)\n",
    "        enc_output, *_ = self.encoder(context, mask)\n",
    "        lin_output = self.relu(self.linear1(rearrange(enc_output,'b s d -> b (s d)')))\n",
    "        seq_logit = self.relu(self.linear2(lin_output))\n",
    "        \n",
    "        return seq_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERROR logs when building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 1 : nn.embedding() 은 input Tensor가 int 형이어야한다.\n",
    "c:\\Users\\jaeng\\Desktop\\VSC\\DACON\\text_classification\\main.ipynb 셀 23 in TransformerClassifier.forward(self, context)\n",
    "...\n",
    "   2197     # remove once script supports set_grad_enabled\n",
    "   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
    "-> 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
    "\n",
    "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
    "\n",
    "------------------------\n",
    "\n",
    "학습 해보려 하는데,\n",
    "- 모델 내부에서 int로 받아야하는 'indices'라는 놈에 floatTensor 가 들어가고있어 말썽인 상황이다. 내일 해결해봐야지.\n",
    "- torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 의 첫번째 argument weight가 Int이어야하는데 Float로 들어온건가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 2: \n",
    "-> 3014 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
    "\n",
    "IndexError: Target -9223372036854775808 is out of bounds.\n",
    "\n",
    "model 어딘가에서 normalization 일어나고있지 않나?\n",
    "linear 결과에 norm 추가해야하나? 다른 모델 봐야겠다.\n",
    "- Y 찍어보니 그 안에 엄청 큰 절댓값이 존재한다.. 뭐지?\n",
    "\n",
    "-1.9357e+16 이 값이, -19356592969351168 이 된다. 왜 저 값이 존재하지?\n",
    "Y label 만드는 과정에서 오류가 있을거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 3: model 과 Tensor 의 .to(device) 함수 차이\n",
    "https://stackoverflow.com/questions/59560043/what-is-the-difference-between-model-todevice-and-model-model-todevice\n",
    "\n",
    "---> 36 train(classifier_model, train_loader, optimizer, device, criterion)\n",
    "AttributeError: 'TransformerClassifier' object has no attribute 'device'\n",
    "\n",
    "Model can be placed in GPU with code,\n",
    "```\n",
    "a = my_model()  # a is in cpu\n",
    "a.to(device)    # a is moved to gpu\n",
    "```\n",
    "\n",
    "But Tensor cannot be moved to GPU with the same code.\n",
    "```\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.to(device)    # a is in cpu\n",
    "a = a.to(devivce)   # a is now in gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 4: \n",
    "RuntimeError: CUDA error: no kernel image is available for execution on the device\n",
    "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
    "\n",
    "https://ndb796.tistory.com/744  을 보고 해결했다.\n",
    "- 현재 커널의 python 버전과, CUDA 버전이 호환되지 않아서 발생하는 문제다.\n",
    "- python 버전을 3.6 -> 3.8 업그레이드함으로 해결했다.(사실은 conda 환경을 새로 팜)\n",
    "    - 내가 가진 CUDA버전이 3.6이랑 호환이 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR 5: 모델 반환모양이 ( b seq class ) 이다. ( b c ) 이어야하는데..?\n",
    "\n",
    "in model forward... context.shape torch.Size([64, 30])\n",
    "in model forward... mask.shape torch.Size([64, 1, 30])\n",
    "in model forward... enc_output.shape torch.Size([64, 30, 512])      # linear 을 잘못 수행하였다.\n",
    "in model forward... after_linear.shape torch.Size([64, 30, 7])\n",
    "-> linear 추가해서, 한번 더 거치게 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    \"BATCH_SIZE\":64,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'],\n",
    "                                        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                        n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = classifier_model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_PATH = './ckpt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, loss_fn):\n",
    "    \"\"\"\n",
    "        input: model, data_loader, loss_fn\n",
    "        output: loss, acc\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:    # ERROR1: \n",
    "            # print('================INSIDE evaluate()==================\\n\\n')\n",
    "            # print('what eval_loader gives: ',batch)\n",
    "            X = batch[0]\n",
    "            gt = batch[1].to(device)\n",
    "            Y = torch.argmax(batch[1], dim=1)\n",
    "            # print('Y exists in the form of: ', Y)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            output = model(X)\n",
    "            epoch_loss += loss_fn(output, Y).item()\n",
    "            epoch_acc += binary_accuracy(output, gt).item()\n",
    "            # print('================================================\\n\\n')\n",
    "    \n",
    "    # print('================INSIDE evaluate()==================\\n\\n')\n",
    "    # print('epoch_loss: ', epoch_loss)\n",
    "    # print('epoch_ascc: ', epoch_acc)\n",
    "    # print('epoch_loss_compute', epoch_loss/len(eval_loader))\n",
    "    # print('epoch_acc_compute', epoch_acc/len(eval_loader))\n",
    "    # print('epoch_loss_type', type(epoch_loss/len(eval_loader)))\n",
    "    # print('epoch_acc_type', type(epoch_acc/len(eval_loader)))\n",
    "    # print('================================================\\n\\n')\n",
    "    return epoch_loss/len(eval_loader), epoch_acc/len(eval_loader)\n",
    "\n",
    "\n",
    "def binary_accuracy(pred, gt):\n",
    "    # pred (B score)\n",
    "    # gt (B binary_value)\n",
    "    pred_class = torch.argmax(pred, dim=1)  # (B, )\n",
    "    gt_class = torch.argmax(gt, dim=1)      # (B, )\n",
    "    result = pred_class==gt_class\n",
    "    \n",
    "    return result.sum()/len(result)\n",
    "\n",
    "def show_me_what_you_got(pred,gt):\n",
    "    \"\"\"\n",
    "    Just to validate the accuracy by my eyes.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        '\\n********************** SHOW ME WHAT YOU GOT, MODEL. **********************'\n",
    "    )\n",
    "    pred_class = torch.argmax(pred, dim=1)  # (B,)\n",
    "    gt_class = torch.argmax(gt, dim=1)\n",
    "    print('pred_class: ',pred_class)\n",
    "    print('gt_class: ', gt_class)\n",
    "    result = pred_class==gt_class\n",
    "    print('Acc: ',result.sum()/len(result))\n",
    "    print(\n",
    "        '********************************* THANKS *********************************\\n'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, device, criterion, ckpt_path):\n",
    "    model.to(device)\n",
    "    criterion = criterion.to(device)   # loss\n",
    "    best_val_loss = 1e5\n",
    "\n",
    "    for epoch in range(1,CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        for X,Y in tqdm(train_loader, desc='Training ...'):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            acc = binary_accuracy(output, Y)    # acc for each batch -- is this right..?\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(acc.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch}  |  Train Loss : [{np.mean(train_loss):.5f}]  |  Train Acc: [{np.mean(train_acc):5f}] \")\n",
    "        \n",
    "        # Check if metric works well (per epoch)\n",
    "        # show_me_what_you_got(output, Y)\n",
    "        # If Validation \n",
    "        if valid_loader!=None:\n",
    "            \n",
    "            val_loss, val_acc = evaluate(model, valid_loader, criterion)\n",
    "            \n",
    "            print(f\"Epoch {epoch}  |  Valid Loss : [{np.mean(val_loss):.5f}]  |  Valid Acc: [{np.mean(val_acc):5f}] \")\n",
    "            # Best Model\n",
    "            if val_loss < best_val_loss: \n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(ckpt_path, 'best_model.ckpt'))\n",
    "                print('============= Best model saved! =============')\n",
    "        \n",
    "        # Model Saving\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_path, f'model_epoch{epoch}.ckpt'))\n",
    "        print('================ model saved ================')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple trianing for whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'],\n",
    "                                        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                        n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "optimizer = torch.optim.Adam(params = classifier_model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ckpt_path = os.path.join(MODEL_PATH, 'simple_train')\n",
    "\n",
    "train(\n",
    "    model=classifier_model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=None,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    ckpt_path=ckpt_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOUND ERROR\n",
    "\n",
    "accuracy 평가방식을 완전 잘못 구현했었다. 그래서 틀린게 많을수록 이상한 acc가 높게 나왔던 것이라 생각된다.<br/>\n",
    "-> 결과에 argmax를 안하고 비교를 해서 엉망진창이었다. 해결!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simaple Train Data - Test Data Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        for X, in tqdm(test_loader, desc='Inferencing ...'):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            preds += pred.cpu().tolist()\n",
    "    \n",
    "    # pd.DataFrame(preds).to_csv('test_result.csv')\n",
    "    result2classidx(preds)\n",
    "    print('======= Saved as \\'test_result.csv\\' =======')\n",
    "\n",
    "def result2classidx(preds):\n",
    "    \"\"\"\n",
    "    preds (N class_num=7)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i, data_idx in enumerate(range(45654, 54785)):\n",
    "        topic_index = np.argmax(preds[i])\n",
    "        # interrupt : test결과보고, train에서 truth랑 비교하는부분 내가 argmax 신경 잘썼는지 체크해봐야겠다 생각.\n",
    "t = [1,2,3,4,5]\n",
    "result2classidx(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\n",
    "    model = classifier_model,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-fold Data - training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f'Fold {fold} Processing ...')\n",
    "    \n",
    "    classifier_model = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'],\n",
    "                                        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                        n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "    optimizer = torch.optim.Adam(params = classifier_model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset[fold], batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset[fold], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    ckpt_path = os.path.join(MODEL_PATH, f'fold{fold}')\n",
    "    \n",
    "    train(model=classifier_model, train_loader=train_loader,valid_loader=val_loader, \n",
    "          optimizer=optimizer, device=device, criterion=criterion, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERRORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ERROR 1 : ValueError: not enough values to unpack (expected 2, got 1)\n",
    "- it was 코드 오타\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERROR 1 : Loss[1.94591], Acc[1.105499],[1.105548] for \"fold1,4\"\n",
    "- Train Loss, Valid Loss 값이 똑같게 나온다.\n",
    "- Accuracy 는 다르지만, 거의 비슷한 수준이다.\n",
    "\n",
    "1. 아예 같은 Loss 가 나온다는 것 은, 같은 데이터가 들어갔다는 것 이고,\n",
    "2. 다른 데이터셋인데 Train, Valid Loss가 같다는것은, --> \n",
    "    - weight 들이 .eval() 모드에서 잘 loaded 됐나?\n",
    "3. 한 데이터셋에서 (Valid)Acc가 같다는 것은, --> epoch마다 train data를 통해 학습이 전혀 이뤄지지 않았다는 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issues 1\n",
    "1. 왜 epoch10 이후로 성능이 확 감소한거지? train에 overfitting 된것도 아니고, lr 문제인가?\n",
    "    - acc 산식이 완전 틀렸었다.\n",
    "2. 왜 inital training 시, accuracy 가 train, valid 둘 다 50%에 달하는거지?  binary classification도 아니고, \n",
    "    - acc 산식이 완전 틀렸었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "http://www.dinnopartners.com/__trashed-4/ 참조함.\n",
    "\n",
    "1. 각 모델 load\n",
    "2. 각 모델 predict\n",
    "    - 1,4 모델 reliability check 선행\n",
    "3. 각 모델 Hard Voting -> generalized prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb 셀 49\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m LEARNED_MODEL_PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./ckpt/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model_0 \u001b[39m=\u001b[39m TransformerClassifier(n_vocab\u001b[39m=\u001b[39m\u001b[39m32000\u001b[39m, pad_idx\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab[\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                 d_word_vec\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, d_model\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, d_inner\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                 n_layers\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, n_head\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, d_k\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, n_position\u001b[39m=\u001b[39mMAX_LEN)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_0\u001b[39m.\u001b[39;49mload_state_dict(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(LEARNED_MODEL_PATH, \u001b[39m'\u001b[39;49m\u001b[39mfold0\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mbest_model.ckpt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kist/Desktop/JH/dacon/DACON_text_classification/main.ipynb#Y151sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model_0\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/dacon/lib/python3.9/site-packages/torch/nn/modules/module.py:1470\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[39m# copy state_dict so _load_from_state_dict can modify it\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(state_dict, \u001b[39m'\u001b[39m\u001b[39m_metadata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1470\u001b[0m state_dict \u001b[39m=\u001b[39m state_dict\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m   1471\u001b[0m \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1472\u001b[0m     \u001b[39m# mypy isn't aware that \"_metadata\" exists in state_dict\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     state_dict\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m metadata  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# 1. 각 모델 loading\n",
    "import os\n",
    "\n",
    "LEARNED_MODEL_PATH = './ckpt/'\n",
    "\n",
    "model_0 = TransformerClassifier(n_vocab=32000, pad_idx=tokenizer.vocab['[PAD]'],\n",
    "                                d_word_vec=512, d_model=512, d_inner=2048,\n",
    "                                n_layers=6, n_head=8, d_k=64, dropout=0.1, n_position=MAX_LEN)\n",
    "model_0.load_state_dict(os.path.join(LEARNED_MODEL_PATH, 'fold0','best_model.ckpt'))\n",
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        for X, in tqdm(test_loader, desc='Inferencing ...'):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            preds += pred.cpu().tolist()\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(data_df=test_data, tokenizer=tokenizer, max_seq_len=MAX_LEN, pad_idx = 1, mode='test')\n",
    "\n",
    "# make test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: 가장 좋은 모델 ckpt 가져와서 load 해줘야하지만 지금은 대충 train 한거 기억하는거 그대로 가져온다.\n",
    "# 대충 하는중\n",
    "model = classifier_model\n",
    "preds = predict(model, test_loader, device)\n",
    "\n",
    "preds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dacon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ec7f357a115716716467a0d0135356196f517069ab3d4b1cdbe81d3a8a5bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
